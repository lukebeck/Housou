import csv
import MeCab
from collections import OrderedDict

import os

file_path = os.path.dirname(os.path.realpath(__file__))
csv_file = file_path + '/articles.csv'

def analyse():
    entries = []
    articles = 0
    with open(csv_file, 'r') as myfile:
        reader = csv.reader(myfile, delimiter=',')
        for row in reader:
            entries.append(row[2])
            articles +=1

    occurances = {}
    morphemes = 0
    # For neologd pass '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd' to Tagger()
    tagger = MeCab.Tagger()
    for entry in entries:
        parsed = tagger.parseToNode(entry)
        while parsed:
            morphemes += 1
            lemma = parsed.feature.split(',')[6]
            if lemma == '*':
                pass
            elif lemma in occurances:
                occurances[lemma] += 1
            else:
                occurances[lemma] = 1
            parsed = parsed.next

    morpheme = OrderedDict(sorted(occurances.items(),key=lambda t: t[0]))
    count = OrderedDict(sorted(occurances.items(),key=lambda t: t[1],reverse=True))

    # Just change to words for word ordered output
    output_count = []
    for i in count:
        row = [i,count[i]]
        output_count.append(row)

    output_morpheme = []
    for i in morpheme:
        row = [i,morpheme[i]]
        output_morpheme.append(row)

    def table(output):
        header = 'Morpheme | Count\n'
        divider = '--- | ---:\n'
        table = ''
        for x in output:
            table = table + f'{x[0]} | {x[1]}\n'
        return header + divider + table

    analytics = f'''# Housou-data

This directory contains the data generated by the [Housou Project](https://github.com/lukebeck/housou), 
which collects information on morpheme frequency from [NHK](https://www3.nhk.or.jp/news/).

All data is currently raw, containing entries including `Á∂ö„Åç„ÇíË™≠„ÇÄ`, punctuation marks, specific times, dates, etc.

As this is a new project, current frequency ratings are unrepresentative, as can be seen by the prominience of words 
like Âè∞È¢®12Âè∑. Parsing more articles will not solve this issue, only doing so over a longer period of time.

## üîñ Data
File | Description
--- | ---
[`count.csv`](count.csv) | morpheme data ordered by count
[`morpheme.csv`](morpheme.csv) | morpheme data ordered by morpheme
[`table.md`](table.md) | markdown document tabling data for top 2000 entries ordered by count

## üîñ Statistics

- **Articles analysed:** {articles} 
- **Morphemes identified:** {morphemes}
- **Unique morphemes:** {len(output_morpheme)}

## üîñ 20 Most Frequent Morphemes
    '''

    with open(file_path + '/housou-data/table.md', 'w') as myfile:
        myfile.write(table(output_count[:2000]))

    with open(file_path + '/housou-data/README.md', 'w') as myfile:
        myfile.write(analytics)
        myfile.write(table(output_count[:20]))

    with open(file_path + '/housou-data/count.csv', 'w') as myfile:
        wr = csv.writer(myfile)
        for row in output_count:
            wr.writerow(row)

    with open(file_path + '/housou-data/morpheme.csv', 'w') as myfile:
        wr = csv.writer(myfile)
        for row in output_morpheme:
            wr.writerow(row)

    print(f'''‚ñ≥ Articles analysed: {articles} 
‚ñ≥ Morphemes identified: {morphemes}
‚ñ≥ Unique morphemes: {len(output_morpheme)}
‚ñ≥ Most frequency morphemes:''')

    for x, y in output_count[:7]:
        print(y,x)